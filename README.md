# nli-data-pipelines

Data Pipelines for NLI Data

## Usage

Using Docker is the easiest way to run the pipelines

Just install Docker and Docker Compose and you're good to go

If you are using Ubuntu (Or similar Linux) - try the install_docker.sh script

Once Docker is installed, you can start the pipelines server:

```
docker-compose up -d pipelines
```

You can see pipelines status at http://localhost:5000/

You can also run the pipelines manually, see the list of available pipelines:

```
docker-compose run pipelines
```

Run a pipeline

```
docker-compose run pipelines run ./collections-root
```

Data generated by the pipelines should be available under `data` directory

## Local Installation

If you want to run the pipelines locally (not inside docker) you will need to install some dependencies

Following snippet should install most of the required dependencies

```
sudo apt-get install python3.6 python3-pip python3.6-dev libleveldb-dev libleveldb1v5
sudo pip3 install pipenv
```

We use pipenv to provide consistent and secure Python dependencies, you don't need to setup a virtualenv or anything else

To install the package dependencies and setup the virtualenv:

```
cd nli-data-pipelines
pipenv install
```

You can now run pipenv commands locally
